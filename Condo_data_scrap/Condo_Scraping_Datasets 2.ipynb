{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88509b72",
   "metadata": {},
   "source": [
    "## <h1 align = \"middle\">Programming for Data Science</h1>\n",
    "* **Lecturer : Chan Sophal**\n",
    "* **Group 5**\n",
    "1. Sao Samarth    ID: e20200084\n",
    "2. Sang Rithpork  ID: e20200232\n",
    "3. Set Sophy      ID: e20201576\n",
    "4. Sok Sreyseey   ID: e20201226\n",
    "5. Song Phalla    ID: e20200439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac2ec02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup               # Import BeautifulSoup library in order to scrape information from webpages        \n",
    "import requests                                     # Import requests library in order to send HTTP requests to grap these webpages\n",
    "import pandas as pd                                 # Import pandas library in order to know concept as a DataFrame(can hold data and be easily manipulated)\n",
    "from urllib.request import Request, urlopen         # urllib.request for opening and reading url\n",
    "                                                    # Request sends a request of the specified method to the specified url\n",
    "\n",
    "datasets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3daade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url):                                      # URL the address of a resource (such as a document or website) on the Internet\n",
    "    \n",
    "    response = requests.get(url)                        # Request to webpage\n",
    "    pages = soup(response.text, \"lxml\")                 # Lxml handling of XML and HTML(HyperText Markup Language) files\n",
    "                                                        # BeautifulSoup allows to parse HTML and XML documents and extract data from them\n",
    "    condos = pages.find_all(\"div\", class_ = \"item css-1uuzwjq eq4or9x0\")      # Find all points in \"div\" that has class_ = \"item css-1uuzwjq eq4or9x0\" as parents class\n",
    "    \n",
    "    for condo in condos:        \n",
    "        \n",
    "        item_info = {}\n",
    "        item_info[\"Price\"] = condo.find(\"span\", class_ = \"value\").string\n",
    "        item_info[\"Location\"] = condo.find(\"div\", class_ = \"address\").text[1:]\n",
    "        item_info[\"Bedroom\"] = \"N/A\"       # If not found print N/A     \n",
    "        item_info[\"Bathroom\"] = \"N/A\"\n",
    "        item_info[\"Floor_area\"] = \"N/A\"\n",
    "        item_info[\"Time\"] = \"N/A\"\n",
    "        \n",
    "        try:               # Test a block of code for errors \n",
    "            \n",
    "            Upload = condo.find(\"div\", class_ = \"listed\")\n",
    "            item_info[\"Time\"] = str(Upload).split(\"<span>\")[0].split(\": \")[1].split(\">\")[1]\n",
    "\n",
    "            span = condo.find_all(\"span\", class_ = \"label\")\n",
    "            \n",
    "            for feature in span:\n",
    "                \n",
    "                if \"Bedrooms: \" in feature.text:\n",
    "                    item_info[\"Bedroom\"] = str(feature.text.replace(\"Bedrooms: \", \" \"))\n",
    "                elif \"Bathrooms: \" in feature.text:\n",
    "                    item_info[\"Bathroom\"] = str(feature.text.replace(\"Bathrooms: \", \" \"))\n",
    "                elif \"Floor area: \" in feature.text:\n",
    "                    item_info[\"Floor_area\"] = str(feature.text.replace(\"Floor area: \", \" \"))\n",
    "                    \n",
    "        except:           # Handle the error\n",
    "                          # If the try block raises an error, the except block will be executed\n",
    "            print(end = \" \")\n",
    "        datasets.append(item_info)         # Add item_info into list datasets[]\n",
    "        \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2163f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(datasets):\n",
    "    \n",
    "    df = pd.DataFrame(datasets, columns = [   \"Price\", \"Location\", \"Bedroom\", \"Bathroom\", \"Floor_area\", \"Time\"])\n",
    "    print(df)\n",
    "    df.to_csv(\"Condo_Datasets_for_Sale_from_Realestate.csv\")          # Pandas data frame can use df.to_csv() to export data in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a60e00a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many pages do you want to scrape? : 40\n",
      "        Price                                           Location Bedroom  \\\n",
      "0    $160,000              Chak Angrae Leu, Meanchey, Phnom Penh       2   \n",
      "1    $200,000              Chak Angrae Leu, Meanchey, Phnom Penh       3   \n",
      "2    $115,000              Chak Angrae Leu, Meanchey, Phnom Penh       2   \n",
      "3    $185,000              Chak Angrae Leu, Meanchey, Phnom Penh       3   \n",
      "4     $80,000              Chak Angrae Leu, Meanchey, Phnom Penh       1   \n",
      "..        ...                                                ...     ...   \n",
      "775   $99,999                    Veal Vong, 7 Makara, Phnom Penh       1   \n",
      "776  $132,110  Properties Dabest, Nirouth, Chbar Ampov, Phnom...       2   \n",
      "777  $114,000  Dabest Properties, Nirouth, Chbar Ampov, Phnom...       1   \n",
      "778   $40,000                Boeung Kak 1, Toul Kork, Phnom Penh     N/A   \n",
      "779  $160,848                      BKK 1, Chamkarmon, Phnom Penh       2   \n",
      "\n",
      "    Bathroom Floor_area         Time  \n",
      "0          2       90m²  2 weeks ago  \n",
      "1          3      110m²  2 weeks ago  \n",
      "2          2       85m²  2 weeks ago  \n",
      "3          2      120m²  2 weeks ago  \n",
      "4          1       51m²  2 weeks ago  \n",
      "..       ...        ...          ...  \n",
      "775        1       53m²  2 weeks ago  \n",
      "776        1       74m²  2 weeks ago  \n",
      "777        1       65m²  2 weeks ago  \n",
      "778      N/A        N/A  2 weeks ago  \n",
      "779        1        N/A  2 weeks ago  \n",
      "\n",
      "[780 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':                            # Set __name__ as main class\n",
    "    \n",
    "    count = 0\n",
    "    pages = int(input(\"How many pages do you want to scrape? : \"))\n",
    "    \n",
    "    for page in range(1, pages):\n",
    "        \n",
    "        count = count + 1\n",
    "        data = get_data(f\"https://www.realestate.com.kh/buy/condo/?page={pages}\")            # Find datasets from the first page until the number of pages that you input - 1 \n",
    "        \n",
    "    export_data(datasets)          # Calling export_data function in order to export datasets as CSV file and print datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e612439",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "price = []\n",
    "location = []\n",
    "description = []\n",
    "time = []\n",
    "    \n",
    "for k in range(0, 100):\n",
    "    \n",
    "    url = \"https://www.khmer24.com/en/property/condo-for-sale.html?per_page=\" + str(k)      \n",
    "\n",
    "    request = Request(url, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\"})\n",
    "    webpages = urlopen(request).read()           # Open request and read\n",
    "    page = soup(webpages, \"html.parser\")\n",
    "    info = page.findAll('div', class_ = 'item-detail')    \n",
    "    \n",
    "    for scrape in info:\n",
    "        \n",
    "        try:\n",
    "            title.append(scrape.find('h2', {'class':'item-title'}).text.replace('\\n', \"------\"))\n",
    "        except:\n",
    "            title.append('n/a')\n",
    "        try:\n",
    "            price.append(scrape.find('span', {'class':'price'}).string)\n",
    "        except:\n",
    "            price.append('n/a')\n",
    "        try:\n",
    "            location.append(scrape.find('li').text)\n",
    "        except:\n",
    "            location.append('n/a')\n",
    "        try:\n",
    "            description.append(scrape.find('p',{'class':'description'}).text.replace('\\n', \"\"))\n",
    "        except:\n",
    "            description.append('n/a')\n",
    "        try:\n",
    "            time.append(scrape.find('ul',{'class':'list-unstyled'}).find('time').text)\n",
    "        except:\n",
    "            time.append('n/a')\n",
    "                         \n",
    "                         \n",
    "\n",
    "condo_data = pd.DataFrame({'Title':title,\n",
    "                           'Price':price,\n",
    "                           'Location':location,\n",
    "                           'Description':description,\n",
    "                           'Posted_time':time})\n",
    "\n",
    "print(condo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_data.to_csv('Condo_Datasets_for_Sale_from_Khmer24.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d230fac",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
